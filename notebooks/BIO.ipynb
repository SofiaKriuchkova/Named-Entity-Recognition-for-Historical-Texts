{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1d3c57-3d44-4ea4-8cf9-b10667392040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import regex as re\n",
    "from estnltk import Text, Layer\n",
    "from estnltk.taggers.standard.text_segmentation.compound_token_tagger import CompoundTokenTagger, ALL_1ST_LEVEL_PATTERNS\n",
    "from estnltk.taggers.standard.text_segmentation.patterns import MACROS\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5631bac4-0094-44d4-9cd7-26975c9328d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Filtering JSON files...\n",
      "âœ“ Filtered 44 files to json_ne_gold_a\n",
      "\n",
      "Step 2: Running tokenization diagnostics on ALL spans...\n",
      "\n",
      "================================================================================\n",
      "TOTAL: 22660 spans, 0 missing alignments (0.0%)\n",
      "================================================================================\n",
      "âœ“ All spans written to: all_spans.tsv\n",
      "\n",
      "âœ“ Done!\n",
      "  - all_spans.tsv: Contains ALL spans with their tokenization\n",
      "  - tokenization_errors.tsv: Contains only problematic spans\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NER Preprocessing Pipeline for Tartu City Council Protocols\n",
    "Handles filtering, tokenization correction, and diagnostics\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = \"../data/raw\"  # Input: original annotated JSONs\n",
    "FILTERED_DIR = \"json_ne_gold_a\"           # Output: filtered JSONs (ne_gold_a only)\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZATION CORRECTION (from words_tokenization.py)\n",
    "# ============================================================================\n",
    "\n",
    "def make_adapted_cp_tagger(**kwargs):\n",
    "    \"\"\"Creates an adapted CompoundTokenTagger that:\n",
    "       1) excludes roman numerals from names with initials;\n",
    "       2) does not join date-like token sequences as numbers;\n",
    "    \"\"\"\n",
    "    # Pattern 1: Names with 2 initials (exclude titles and roman numerals I, V, X)\n",
    "    redefined_pat_1 = {\n",
    "        'comment': '*) Names starting with 2 initials (exclude titles and roman numerals I, V, X);',\n",
    "        'pattern_type': 'name_with_initial',\n",
    "        'example': 'A. H. Tammsaare',\n",
    "        '_regex_pattern_': re.compile(r'''\n",
    "            (?!(Dr\\.|Lb\\.|Lh\\.|Lm\\.|Ln\\.|Lv\\.|Lw\\.|Pr\\.))     # exclude titles\n",
    "            ([ABCDEFGHJKLMNOPQRSTUWYZÅ Å½Ã•Ã„Ã–Ãœ][{LOWERCASE}]?)   # first initial\n",
    "            \\s?\\.\\s?-?                                        # period (and hyphen potentially)\n",
    "            ([ABCDEFGHJKLMNOPQRSTUWYZÅ Å½Ã•Ã„Ã–Ãœ][{LOWERCASE}]?)   # second initial\n",
    "            \\s?\\.\\s?                                          # period\n",
    "            ((\\.[{UPPERCASE}]\\.)?[{UPPERCASE}][{LOWERCASE}]+) # last name\n",
    "        '''.format(**MACROS), re.X),\n",
    "        '_group_': 0,\n",
    "        '_priority_': (4, 1),\n",
    "        'normalized': lambda m: re.sub('\\1.\\2. \\3', '', m.group(0)),\n",
    "    }\n",
    "\n",
    "    # Pattern 2: Names with 1 initial (exclude roman numerals I, V, X)\n",
    "    redefined_pat_2 = {\n",
    "        'comment': '*) Names starting with one initial (exclude roman numerals I, V, X);',\n",
    "        'pattern_type': 'name_with_initial',\n",
    "        'example': 'A. Hein',\n",
    "        '_regex_pattern_': re.compile(r'''\n",
    "            ([ABCDEFGHJKLMNOPQRSTUWYZÅ Å½Ã•Ã„Ã–Ãœ])   # first initial\n",
    "            \\s?\\.\\s?                            # period\n",
    "            ([{UPPERCASE}][{LOWERCASE}]+)       # last name\n",
    "        '''.format(**MACROS), re.X),\n",
    "        '_group_': 0,\n",
    "        '_priority_': (4, 2),\n",
    "        'normalized': lambda m: re.sub('\\1. \\2', '', m.group(0)),\n",
    "    }\n",
    "\n",
    "    # Pattern 3: Long numbers (1 group, corrected for timex tagger)\n",
    "    redefined_number_pat_1 = {\n",
    "        'comment': '*) A generic pattern for detecting long numbers (1 group) (corrected for timex tagger).',\n",
    "        'example': '12,456',\n",
    "        'pattern_type': 'numeric',\n",
    "        '_group_': 0,\n",
    "        '_priority_': (2, 1, 5),\n",
    "        '_regex_pattern_': re.compile(r'''                             \n",
    "            \\d+           # 1 group of numbers\n",
    "            (,\\d+|\\ *\\.)  # + comma-separated numbers or period-ending\n",
    "        ''', re.X),\n",
    "        'normalized': r\"lambda m: re.sub(r'[\\s]' ,'' , m.group(0))\"\n",
    "    }\n",
    "\n",
    "    # Pattern 4: Long numbers (2 groups, point-separated, followed by comma-separated)\n",
    "    redefined_number_pat_2 = {\n",
    "        'comment': '*) A generic pattern for detecting long numbers (2 groups, point-separated, followed by comma-separated numbers) (corrected for timex tagger).',\n",
    "        'example': '67.123,456',\n",
    "        'pattern_type': 'numeric',\n",
    "        '_group_': 0,\n",
    "        '_priority_': (2, 1, 3, 1),\n",
    "        '_regex_pattern_': re.compile(r'''\n",
    "            \\d+\\.+\\d+   # 2 groups of numbers\n",
    "            (,\\d+)      # + comma-separated numbers\n",
    "        ''', re.X),\n",
    "        'normalized': r\"lambda m: re.sub(r'[\\s\\.]' ,'' , m.group(0))\"\n",
    "    }\n",
    "\n",
    "    # Build new pattern list\n",
    "    new_1st_level_patterns = []\n",
    "    for pat in ALL_1ST_LEVEL_PATTERNS:\n",
    "        # Skip these patterns\n",
    "        if pat['comment'] in [\n",
    "            '*) Abbreviations of type <uppercase letter> + <numbers>;',\n",
    "            '*) Date patterns that contain month as a Roman numeral: \"dd. roman_mm yyyy\";',\n",
    "            '*) Date patterns in the commonly used form \"dd/mm/yy\";'\n",
    "        ]:\n",
    "            continue\n",
    "        \n",
    "        # Replace these patterns\n",
    "        if pat['comment'] == '*) Names starting with 2 initials;':\n",
    "            new_1st_level_patterns.append(redefined_pat_1)\n",
    "        elif pat['comment'] == '*) Names starting with one initial;':\n",
    "            new_1st_level_patterns.append(redefined_pat_2)\n",
    "        elif pat['comment'] == '*) A generic pattern for detecting long numbers (1 group).':\n",
    "            new_1st_level_patterns.append(redefined_number_pat_1)\n",
    "        elif pat['comment'] == '*) A generic pattern for detecting long numbers (2 groups, point-separated, followed by comma-separated numbers).':\n",
    "            new_1st_level_patterns.append(redefined_number_pat_2)\n",
    "        else:\n",
    "            new_1st_level_patterns.append(pat)\n",
    "    \n",
    "    assert len(new_1st_level_patterns) + 3 == len(ALL_1ST_LEVEL_PATTERNS)\n",
    "    \n",
    "    if kwargs and 'patterns_1' in kwargs:\n",
    "        raise ValueError(\"Cannot overwrite 'patterns_1' in adapted CompoundTokenTagger.\")\n",
    "    \n",
    "    return CompoundTokenTagger(\n",
    "        patterns_1=new_1st_level_patterns,\n",
    "        do_not_join_on_strings=('\\n\\n', '\\n'),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the adapted tagger\n",
    "adapted_cp_tokens_tagger = make_adapted_cp_tagger(\n",
    "    input_tokens_layer='tokens',\n",
    "    output_layer='compound_tokens'\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_words(input_text):\n",
    "    \"\"\"Pre-processes Text object: adds word segmentation.\"\"\"\n",
    "    input_text.tag_layer('tokens')\n",
    "    adapted_cp_tokens_tagger.tag(input_text)\n",
    "    input_text.tag_layer('words') \n",
    "    return input_text\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: FILTER JSON FILES (keep only ne_gold_a or ne_gold_b)\n",
    "# ============================================================================\n",
    "\n",
    "def filter_json_to_gold_a(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Filter JSON files to keep only ne_gold_a layer (or ne_gold_b if ne_gold_a missing).\n",
    "    Prioritizes first annotator (ne_gold_a) over second (ne_gold_b).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for fname in sorted(os.listdir(input_dir)):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "        \n",
    "        input_path = os.path.join(input_dir, fname)\n",
    "        output_path = os.path.join(output_dir, fname)\n",
    "        \n",
    "        with open(input_path, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        layers = data.get(\"layers\", [])\n",
    "        \n",
    "        # Prioritize ne_gold_a over ne_gold_b\n",
    "        has_a = any(layer['name'] == 'ne_gold_a' for layer in layers)\n",
    "        if has_a:\n",
    "            filtered_layers = [layer for layer in layers if layer[\"name\"] == \"ne_gold_a\"]\n",
    "        else:\n",
    "            filtered_layers = [layer for layer in layers if layer[\"name\"] == \"ne_gold_b\"]\n",
    "        \n",
    "        data[\"layers\"] = filtered_layers\n",
    "        \n",
    "        if not filtered_layers:\n",
    "            print(f\"âš  No NER layer in: {fname}\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ Filtered {len([f for f in os.listdir(input_dir) if f.endswith('.json')])} files to {output_dir}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: DIAGNOSTIC - CHECK TOKENIZATION ALIGNMENT\n",
    "# ============================================================================\n",
    "\n",
    "def diagnose_tokenization(file_path, max_spans=None, verbose=True, all_spans_log=None, error_log=None):\n",
    "    \"\"\"\n",
    "    Check alignment between NER spans and word tokens.\n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "        max_spans: Maximum spans to check (None = all spans)\n",
    "        verbose: Print detailed output\n",
    "        all_spans_log: File handle to write ALL spans to (optional)\n",
    "        error_log: File handle to write errors to (optional)\n",
    "    Returns: (total_spans, missing_alignments)\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = data.get(\"text\", \"\")\n",
    "    txt = Text(text)\n",
    "    txt = preprocess_words(txt)\n",
    "    \n",
    "    # Build word token list\n",
    "    words = []\n",
    "    for w in txt['words']:\n",
    "        norm = w.annotations[0].get('normalized_form') if w.annotations else None\n",
    "        norm = norm or w.text\n",
    "        words.append({\n",
    "            \"text\": w.text,\n",
    "            \"start\": w.start,\n",
    "            \"end\": w.end,\n",
    "            \"norm\": norm\n",
    "        })\n",
    "    \n",
    "    # Find NER layer (ne_gold_a or ne_gold_b)\n",
    "    ne_layer = None\n",
    "    for candidate in (\"ne_gold_a\", \"ne_gold_b\"):\n",
    "        ne_layer = next((l for l in data.get(\"layers\", []) if l[\"name\"] == candidate), None)\n",
    "        if ne_layer:\n",
    "            break\n",
    "    \n",
    "    if not ne_layer:\n",
    "        if verbose:\n",
    "            print(f\"âš  No NER layer in: {file_path}\")\n",
    "        return 0, 0\n",
    "    \n",
    "    spans = ne_layer.get(\"spans\", [])\n",
    "    \n",
    "    def tokens_overlapping(start, end):\n",
    "        return [w for w in words if w[\"start\"] < end and w[\"end\"] > start]\n",
    "    \n",
    "    missing = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{file_path}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    spans_to_check = spans if max_spans is None else spans[:max_spans]\n",
    "    \n",
    "    for span in spans_to_check:\n",
    "        start, end = span[\"base_span\"]\n",
    "        etype = span[\"annotations\"][0].get(\"tag\", \"?\")\n",
    "        etext = text[start:end]\n",
    "        covered = tokens_overlapping(start, end)\n",
    "        \n",
    "        # Get token strings\n",
    "        token_strings = [w[\"norm\"] for w in covered] if covered else []\n",
    "        tokens_str = \"|\".join(token_strings)\n",
    "        has_error = \"YES\" if not covered else \"NO\"\n",
    "        \n",
    "        # Write to all_spans_log if provided\n",
    "        if all_spans_log:\n",
    "            all_spans_log.write(f\"{os.path.basename(file_path)}\\t{repr(etext)}\\t{etype}\\t({start},{end})\\t{tokens_str}\\t{has_error}\\n\")\n",
    "        \n",
    "        # Track and log errors separately\n",
    "        if not covered:\n",
    "            missing += 1\n",
    "            \n",
    "            if error_log:\n",
    "                error_log.write(f\"{os.path.basename(file_path)}\\t{repr(etext)}\\t{etype}\\t({start},{end})\\n\")\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"âš  MISSING: {repr(etext)} â†’ {etype} | span=({start},{end})\")\n",
    "        elif verbose:\n",
    "            print(f\"âœ“ {repr(etext)} â†’ {etype} | tokens={token_strings}\")\n",
    "    \n",
    "    if verbose:\n",
    "        checked_count = len(spans_to_check)\n",
    "        print(f\"\\nðŸ“Š Checked {checked_count}/{len(spans)} spans, missing alignments: {missing}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return len(spans), missing\n",
    "\n",
    "\n",
    "def diagnose_all_files(input_dir, max_spans=None, verbose=True, all_spans_file=\"all_spans.tsv\", error_file=\"tokenization_errors.tsv\"):\n",
    "    \"\"\"\n",
    "    Run diagnostics on all JSON files in directory.\n",
    "    Args:\n",
    "        input_dir: Directory with JSON files\n",
    "        max_spans: Maximum spans per file (None = all spans)\n",
    "        verbose: Print detailed output\n",
    "        all_spans_file: File to write ALL spans to (None = don't write)\n",
    "        error_file: File to write errors only (None = don't write)\n",
    "    \"\"\"\n",
    "    total_spans = 0\n",
    "    total_missing = 0\n",
    "    \n",
    "    all_spans_log = None\n",
    "    error_log = None\n",
    "    \n",
    "    if all_spans_file:\n",
    "        all_spans_log = open(all_spans_file, \"w\", encoding=\"utf-8\")\n",
    "        all_spans_log.write(\"file\\tentity_text\\tentity_type\\tspan_positions\\ttokens\\thas_error\\n\")\n",
    "    \n",
    "    if error_file:\n",
    "        error_log = open(error_file, \"w\", encoding=\"utf-8\")\n",
    "        error_log.write(\"file\\tentity_text\\tentity_type\\tspan_positions\\n\")\n",
    "    \n",
    "    try:\n",
    "        for fname in sorted(f for f in os.listdir(input_dir) if f.endswith(\".json\")):\n",
    "            file_path = os.path.join(input_dir, fname)\n",
    "            spans, missing = diagnose_tokenization(file_path, max_spans, verbose, all_spans_log, error_log)\n",
    "            total_spans += spans\n",
    "            total_missing += missing\n",
    "        \n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"TOTAL: {total_spans} spans, {total_missing} missing alignments ({100*total_missing/total_spans:.1f}%)\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        if all_spans_file:\n",
    "            print(f\"âœ“ All spans written to: {all_spans_file}\")\n",
    "        if error_file and total_missing > 0:\n",
    "            print(f\"âš  Errors written to: {error_file}\")\n",
    "    \n",
    "    finally:\n",
    "        if all_spans_log:\n",
    "            all_spans_log.close()\n",
    "        if error_log:\n",
    "            error_log.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Filter JSON files (keep only ne_gold_a)\n",
    "    print(\"Step 1: Filtering JSON files...\")\n",
    "    filter_json_to_gold_a(DATA_DIR, FILTERED_DIR)\n",
    "    \n",
    "    # Step 2: Run diagnostics on ALL spans and save to files\n",
    "    print(\"\\nStep 2: Running tokenization diagnostics on ALL spans...\")\n",
    "    diagnose_all_files(\n",
    "        FILTERED_DIR, \n",
    "        max_spans=None,  # Check ALL spans\n",
    "        verbose=False,   # Don't print every span (too much output)\n",
    "        all_spans_file=\"all_spans.tsv\",  # Write ALL spans here\n",
    "        error_file=\"tokenization_errors.tsv\"  # Write only errors here\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ“ Done!\")\n",
    "    print(\"  - all_spans.tsv: Contains ALL spans with their tokenization\")\n",
    "    print(\"  - tokenization_errors.tsv: Contains only problematic spans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1eda6e-5130-45e0-ba4d-e98a52b2bcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1.3: Converting corpus to BIO format\n",
      "================================================================================\n",
      "Converting corpus to BIO format...\n",
      "  âœ“ 1918-01-22_manual_annotated_anonymized.json: 146 sentences\n",
      "  âœ“ 1918-12-09_manual_annotated_anonymized.json: 76 sentences\n",
      "  âœ“ 1919-06-16_manual_annotated_anonymized.json: 307 sentences\n",
      "  âœ“ 1919-07-28_manual_annotated_anonymized.json: 186 sentences\n",
      "  âœ“ 1919-08-25_manual_annotated_anonymized.json: 233 sentences\n",
      "  âœ“ 1921-05-19_manual_annotated_anonymized.json: 229 sentences\n",
      "  âœ“ 1921-08-29_manual_annotated_anonymized.json: 207 sentences\n",
      "  âœ“ 1921-09-26_manual_annotated_anonymized.json: 313 sentences\n",
      "  âœ“ 1922-04-24_manual_annotated_anonymized.json: 179 sentences\n",
      "  âœ“ 1922-05-29_manual_annotated_anonymized.json: 475 sentences\n",
      "  âœ“ 1923-09-24_manual_annotated_anonymized.json: 489 sentences\n",
      "  âœ“ 1924-11-26_manual_annotated_anonymized.json: 261 sentences\n",
      "  âœ“ 1925-11-11_manual_annotated_anonymized.json: 231 sentences\n",
      "  âœ“ 1926-02-17_manual_annotated_anonymized.json: 558 sentences\n",
      "  âœ“ 1926-03-17_manual_annotated_anonymized.json: 154 sentences\n",
      "  âœ“ 1926-06-21_manual_annotated_anonymized.json: 69 sentences\n",
      "  âœ“ 1926-06-30_manual_annotated_anonymized.json: 580 sentences\n",
      "  âœ“ 1926-08-31_manual_annotated_anonymized.json: 521 sentences\n",
      "  âœ“ 1927-03-28_manual_annotated_anonymized.json: 647 sentences\n",
      "  âœ“ 1927-10-24_manual_annotated_anonymized.json: 700 sentences\n",
      "  âœ“ 1928-09-24_manual_annotated_anonymized.json: 648 sentences\n",
      "  âœ“ 1928-11-28_manual_annotated_anonymized.json: 707 sentences\n",
      "  âœ“ 1929-11-25_manual_annotated_anonymized.json: 542 sentences\n",
      "  âœ“ 1930-01-24_manual_annotated_anonymized.json: 337 sentences\n",
      "  âœ“ 1930-06-16_manual_annotated_anonymized.json: 513 sentences\n",
      "  âœ“ 1930-10-13_manual_annotated_anonymized.json: 1174 sentences\n",
      "  âœ“ 1931-06-15_manual_annotated_anonymized.json: 900 sentences\n",
      "  âœ“ 1932-01-25_manual_annotated_anonymized.json: 1121 sentences\n",
      "  âœ“ 1932-06-20_manual_annotated_anonymized.json: 1272 sentences\n",
      "  âœ“ 1932-09-26_manual_annotated_anonymized.json: 920 sentences\n",
      "  âœ“ 1933-06-14_manual_annotated_anonymized.json: 1431 sentences\n",
      "  âœ“ 1933-09-25_manual_annotated_anonymized.json: 390 sentences\n",
      "  âœ“ 1934-04-30_manual_annotated_anonymized.json: 632 sentences\n",
      "  âœ“ 1934-10-15_manual_annotated_anonymized.json: 696 sentences\n",
      "  âœ“ 1935-02-25_manual_annotated_anonymized.json: 755 sentences\n",
      "  âœ“ 1935-09-30_manual_annotated_anonymized.json: 892 sentences\n",
      "  âœ“ 1936-04-27_manual_annotated_anonymized.json: 805 sentences\n",
      "  âœ“ 1936-09-07_manual_annotated_anonymized.json: 602 sentences\n",
      "  âœ“ 1936-10-26_manual_annotated_anonymized.json: 302 sentences\n",
      "  âœ“ 1938-06-13_manual_annotated_anonymized.json: 944 sentences\n",
      "  âœ“ 1938-06-27_manual_annotated_anonymized.json: 226 sentences\n",
      "  âœ“ 1940-11-13_manual_annotated_anonymized.json: 37 sentences\n",
      "  âœ“ 1940-12-27_manual_annotated_anonymized.json: 150 sentences\n",
      "  âœ“ 1941-01-03_manual_annotated_anonymized.json: 46 sentences\n",
      "\n",
      "âœ“ Saved BIO corpus to: corpus_bio.tsv\n",
      "  Total sentences: 22603\n",
      "  Total tokens: 287509\n",
      "\n",
      "  Entity counts:\n",
      "    EVENT: 52\n",
      "    LAW: 585\n",
      "    LOC: 3340\n",
      "    MONEY: 2578\n",
      "    ORG: 7789\n",
      "    PER: 5931\n",
      "    POSITION: 2308\n",
      "    UNK: 46\n",
      "\n",
      "================================================================================\n",
      "âœ“ BIO conversion complete!\n",
      "================================================================================\n",
      "Output file: corpus_bio.tsv\n",
      "\n",
      "Next: Run Step 1.4 (train/dev/test split)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1.3: Convert corpus to BIO format\n",
    "Unifies entity categories and converts to BIO tagging format\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "FILTERED_DIR = \"json_ne_gold_a\"           # Input: filtered JSONs\n",
    "BIO_OUTPUT_FILE = \"corpus_bio.tsv\"        # Output: BIO format corpus\n",
    "\n",
    "# Category mapping for unification\n",
    "CATEGORY_MAPPING = {\n",
    "    'LOC_ADDRESS': 'LOC',\n",
    "    'ORG_GPE': 'ORG',\n",
    "    'ORG_POL': 'ORG',\n",
    "    # Keep others as-is: PER, LOC, ORG, POSITION, etc.\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# BIO CONVERSION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def unify_category(category):\n",
    "    \"\"\"Unify entity categories according to mapping.\"\"\"\n",
    "    return CATEGORY_MAPPING.get(category, category)\n",
    "\n",
    "\n",
    "def convert_file_to_bio(file_path):\n",
    "    \"\"\"\n",
    "    Convert a single JSON file to BIO format.\n",
    "    Returns list of sentences, where each sentence is a list of (token, bio_tag) tuples.\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = data.get(\"text\", \"\")\n",
    "    txt = Text(text)\n",
    "    txt = preprocess_words(txt)\n",
    "    \n",
    "    # Tag sentences\n",
    "    txt.tag_layer('sentences')\n",
    "    \n",
    "    # Build word token list with sentence info\n",
    "    words = []\n",
    "    for sent_id, sent_span in enumerate(txt['sentences']):\n",
    "        sent_words = []\n",
    "        for w in txt['words']:\n",
    "            # Check if word is within this sentence\n",
    "            if w.start >= sent_span.start and w.end <= sent_span.end:\n",
    "                norm = w.annotations[0].get('normalized_form') if w.annotations else None\n",
    "                norm = norm or w.text\n",
    "                sent_words.append({\n",
    "                    \"text\": w.text,\n",
    "                    \"norm\": norm,\n",
    "                    \"start\": w.start,\n",
    "                    \"end\": w.end,\n",
    "                    \"sent_id\": sent_id\n",
    "                })\n",
    "        if sent_words:  # Only add non-empty sentences\n",
    "            words.extend(sent_words)\n",
    "    \n",
    "    # Find NER layer\n",
    "    ne_layer = None\n",
    "    for candidate in (\"ne_gold_a\", \"ne_gold_b\"):\n",
    "        ne_layer = next((l for l in data.get(\"layers\", []) if l[\"name\"] == candidate), None)\n",
    "        if ne_layer:\n",
    "            break\n",
    "    \n",
    "    if not ne_layer:\n",
    "        return []\n",
    "    \n",
    "    spans = ne_layer.get(\"spans\", [])\n",
    "    \n",
    "    # Create BIO tags for each word\n",
    "    for word in words:\n",
    "        word['bio_tag'] = 'O'  # Default: outside any entity\n",
    "    \n",
    "    # Process each NER span\n",
    "    for span in spans:\n",
    "        start, end = span[\"base_span\"]\n",
    "        etype = span[\"annotations\"][0].get(\"tag\", \"?\")\n",
    "        etype = unify_category(etype)  # Unify categories\n",
    "        \n",
    "        # Find overlapping words\n",
    "        overlapping = [w for w in words if w[\"start\"] < end and w[\"end\"] > start]\n",
    "        \n",
    "        if overlapping:\n",
    "            # First token gets B- (Beginning)\n",
    "            overlapping[0]['bio_tag'] = f'B-{etype}'\n",
    "            # Rest get I- (Inside)\n",
    "            for w in overlapping[1:]:\n",
    "                w['bio_tag'] = f'I-{etype}'\n",
    "    \n",
    "    # Group words by sentence\n",
    "    sentences = []\n",
    "    current_sent_id = None\n",
    "    current_sent = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word['sent_id'] != current_sent_id:\n",
    "            if current_sent:\n",
    "                sentences.append(current_sent)\n",
    "            current_sent = []\n",
    "            current_sent_id = word['sent_id']\n",
    "        current_sent.append((word['norm'], word['bio_tag']))\n",
    "    \n",
    "    if current_sent:\n",
    "        sentences.append(current_sent)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def convert_corpus_to_bio(input_dir, output_file):\n",
    "    \"\"\"\n",
    "    Convert all JSON files to BIO format and save as TSV.\n",
    "    Format: token<TAB>bio_tag, with double newline between sentences.\n",
    "    Returns: list of all sentences for further processing.\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "    file_sentences = {}  # Track which file each sentence came from\n",
    "    \n",
    "    print(\"Converting corpus to BIO format...\")\n",
    "    \n",
    "    for fname in sorted(f for f in os.listdir(input_dir) if f.endswith(\".json\")):\n",
    "        file_path = os.path.join(input_dir, fname)\n",
    "        sentences = convert_file_to_bio(file_path)\n",
    "        file_sentences[fname] = sentences\n",
    "        all_sentences.extend(sentences)\n",
    "        print(f\"  âœ“ {fname}: {len(sentences)} sentences\")\n",
    "    \n",
    "    # Write to TSV\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in all_sentences:\n",
    "            for token, bio_tag in sentence:\n",
    "                f.write(f\"{token}\\t{bio_tag}\\n\")\n",
    "            f.write(\"\\n\")  # Double newline between sentences\n",
    "    \n",
    "    print(f\"\\nâœ“ Saved BIO corpus to: {output_file}\")\n",
    "    print(f\"  Total sentences: {len(all_sentences)}\")\n",
    "    print(f\"  Total tokens: {sum(len(s) for s in all_sentences)}\")\n",
    "    \n",
    "    # Count entities by type\n",
    "    entity_counts = {}\n",
    "    for sentence in all_sentences:\n",
    "        for token, bio_tag in sentence:\n",
    "            if bio_tag.startswith('B-'):\n",
    "                etype = bio_tag[2:]\n",
    "                entity_counts[etype] = entity_counts.get(etype, 0) + 1\n",
    "    \n",
    "    print(f\"\\n  Entity counts:\")\n",
    "    for etype, count in sorted(entity_counts.items()):\n",
    "        print(f\"    {etype}: {count}\")\n",
    "    \n",
    "    return all_sentences, file_sentences\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1.3: Converting corpus to BIO format\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_sentences, file_sentences = convert_corpus_to_bio(FILTERED_DIR, BIO_OUTPUT_FILE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ“ BIO conversion complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Output file: {BIO_OUTPUT_FILE}\")\n",
    "    print(\"\\nNext: Run Step 1.4 (train/dev/test split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4f736e-088c-4479-bee5-5b30d20e92b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1.4: Splitting corpus into train/dev/test (EVENT/UNK excluded)\n",
      "================================================================================\n",
      "Random seed: 42\n",
      "Excluded entity types: EVENT, UNK\n",
      "Target split: Train ~81%, Dev ~9%, Test ~10%\n",
      "Writing split files...\n",
      "   train.tsv\n",
      "   dev.tsv\n",
      "   test.tsv\n",
      "\n",
      "================================================================================\n",
      "CORPUS STATISTICS\n",
      "================================================================================\n",
      "Split      Protocols    Sentences    Tokens       Entities    \n",
      "--------------------------------------------------------------------------------\n",
      "Train      35           17643        225676       17483       \n",
      "Dev        4            2170         27060        2121        \n",
      "Test       5            2790         34773        2958        \n",
      "--------------------------------------------------------------------------------\n",
      "TOTAL      44           22603        287509       22562       \n",
      "\n",
      "================================================================================\n",
      "PERCENTAGES (by tokens)\n",
      "================================================================================\n",
      "Train       78.49%\n",
      "Dev          9.41%\n",
      "Test        12.09%\n",
      "\n",
      "================================================================================\n",
      "ENTITY COUNTS BY TYPE (EVENT/UNK excluded)\n",
      "================================================================================\n",
      "Type            Train      Dev        Test       Total     \n",
      "--------------------------------------------------------------------------------\n",
      "LAW             418        58         109        585       \n",
      "LOC             2484       361        495        3340      \n",
      "MONEY           2052       175        351        2578      \n",
      "ORG             5975       741        1073       7789      \n",
      "PER             4728       544        659        5931      \n",
      "POSITION        1804       237        267        2308      \n",
      "================================================================================\n",
      " Statistics saved to: corpus_statistics.txt\n",
      " File distribution saved to file_distribution.txt\n",
      "\n",
      " Train/dev/test split complete!\n",
      "\n",
      "Files created:\n",
      "  - train.tsv\n",
      "  - dev.tsv\n",
      "  - test.tsv\n",
      "  - corpus_statistics.txt\n",
      "  - file_distribution.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1.4: Split corpus into train/dev/test sets\n",
    "- Test: 10% of tokens\n",
    "- Dev: 8-10% of tokens\n",
    "- Train: ~80% of tokens\n",
    "Splits by complete protocols (files), not by individual sentences\n",
    "\n",
    "This version EXCLUDES entity types: EVENT and UNK (everywhere):\n",
    "- They are not converted to BIO tags\n",
    "- They are not counted in per-file entity totals\n",
    "- They do not appear in \"ENTITY COUNTS BY TYPE\"\n",
    "- File distribution \"Total entities\" reflects the excluded setting\n",
    "\"\"\"\n",
    "# ====== CONFIG ======\n",
    "TRAIN_FILE = \"train.tsv\"\n",
    "DEV_FILE = \"dev.tsv\"\n",
    "TEST_FILE = \"test.tsv\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.10   # 10% for test\n",
    "DEV_SIZE = 0.09    # 9% for dev\n",
    "# Train will be remaining (~81%)\n",
    "\n",
    "# Exclude these entity types everywhere\n",
    "EXCLUDED_ENTITY_TYPES = {\"EVENT\", \"UNK\"}\n",
    "\n",
    "# NOTE: These come from your environment. Keep as-is.\n",
    "# - FILTERED_DIR\n",
    "# - Text\n",
    "# - preprocess_words\n",
    "\n",
    "\n",
    "def unify_category(category: str) -> str:\n",
    "    \"\"\"Unify entity categories according to mapping.\"\"\"\n",
    "    CATEGORY_MAPPING = {\n",
    "        'LOC_ADDRESS': 'LOC',\n",
    "        'ORG_GPE': 'ORG',\n",
    "        'ORG_POL': 'ORG',\n",
    "    }\n",
    "    return CATEGORY_MAPPING.get(category, category)\n",
    "\n",
    "\n",
    "def get_ne_layer(data: dict):\n",
    "    \"\"\"Get the first available NER gold layer.\"\"\"\n",
    "    for candidate in (\"ne_gold_a\", \"ne_gold_b\"):\n",
    "        layer = next((l for l in data.get(\"layers\", []) if l.get(\"name\") == candidate), None)\n",
    "        if layer:\n",
    "            return layer\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_file_statistics(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get statistics for a single file: tokens, sentences, entities (EXCLUDING EVENT/UNK).\n",
    "    Returns dict with counts.\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text = data.get(\"text\", \"\")\n",
    "    txt = Text(text)\n",
    "    txt = preprocess_words(txt)\n",
    "    txt.tag_layer('sentences')\n",
    "\n",
    "    num_tokens = len(txt['words'])\n",
    "    num_sentences = len(txt['sentences'])\n",
    "\n",
    "    # Count entities excluding EVENT/UNK\n",
    "    ne_layer = get_ne_layer(data)\n",
    "    num_entities = 0\n",
    "    if ne_layer:\n",
    "        for span in ne_layer.get(\"spans\", []):\n",
    "            tag = span.get(\"annotations\", [{}])[0].get(\"tag\", \"?\")\n",
    "            etype = unify_category(tag)\n",
    "            if etype in EXCLUDED_ENTITY_TYPES:\n",
    "                continue\n",
    "            num_entities += 1\n",
    "\n",
    "    return {\n",
    "        'tokens': num_tokens,\n",
    "        'sentences': num_sentences,\n",
    "        'entities': num_entities\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_file_to_bio_sentences(file_path: str):\n",
    "    \"\"\"Convert a file to BIO format sentences (EXCLUDING EVENT/UNK spans).\"\"\"\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text = data.get(\"text\", \"\")\n",
    "    txt = Text(text)\n",
    "    txt = preprocess_words(txt)\n",
    "    txt.tag_layer('sentences')\n",
    "\n",
    "    # Build word list\n",
    "    words = []\n",
    "    for sent_id, sent_span in enumerate(txt['sentences']):\n",
    "        for w in txt['words']:\n",
    "            if w.start >= sent_span.start and w.end <= sent_span.end:\n",
    "                norm = w.annotations[0].get('normalized_form') if w.annotations else None\n",
    "                norm = norm or w.text\n",
    "                words.append({\n",
    "                    \"norm\": norm,\n",
    "                    \"start\": w.start,\n",
    "                    \"end\": w.end,\n",
    "                    \"sent_id\": sent_id,\n",
    "                    \"bio_tag\": 'O'\n",
    "                })\n",
    "\n",
    "    # Apply NER spans\n",
    "    ne_layer = get_ne_layer(data)\n",
    "    if ne_layer:\n",
    "        for span in ne_layer.get(\"spans\", []):\n",
    "            start, end = span[\"base_span\"]\n",
    "            tag = span.get(\"annotations\", [{}])[0].get(\"tag\", \"?\")\n",
    "            etype = unify_category(tag)\n",
    "\n",
    "            # Exclude these types completely\n",
    "            if etype in EXCLUDED_ENTITY_TYPES:\n",
    "                continue\n",
    "\n",
    "            overlapping = [w for w in words if w[\"start\"] < end and w[\"end\"] > start]\n",
    "            if overlapping:\n",
    "                overlapping[0]['bio_tag'] = f'B-{etype}'\n",
    "                for w in overlapping[1:]:\n",
    "                    w['bio_tag'] = f'I-{etype}'\n",
    "\n",
    "    # Group by sentence\n",
    "    sentences = []\n",
    "    current_sent_id = None\n",
    "    current_sent = []\n",
    "\n",
    "    for word in words:\n",
    "        if word['sent_id'] != current_sent_id:\n",
    "            if current_sent:\n",
    "                sentences.append(current_sent)\n",
    "            current_sent = []\n",
    "            current_sent_id = word['sent_id']\n",
    "        current_sent.append((word['norm'], word['bio_tag']))\n",
    "\n",
    "    if current_sent:\n",
    "        sentences.append(current_sent)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_corpus(input_dir: str, test_size=0.10, dev_size=0.09, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split corpus into train/dev/test by files (protocols).\n",
    "    Returns: dict with keys train/dev/test including files, data, stats, entities.\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    # Get all files and their statistics\n",
    "    files_stats = {}\n",
    "    for fname in sorted(f for f in os.listdir(input_dir) if f.endswith(\".json\")):\n",
    "        file_path = os.path.join(input_dir, fname)\n",
    "        stats = get_file_statistics(file_path)\n",
    "        files_stats[fname] = stats\n",
    "\n",
    "    total_tokens = sum(s['tokens'] for s in files_stats.values())\n",
    "    target_test_tokens = int(total_tokens * test_size)\n",
    "    target_dev_tokens = int(total_tokens * dev_size)\n",
    "\n",
    "    # Shuffle files\n",
    "    all_files = list(files_stats.keys())\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    # Greedy allocation to get close to target sizes\n",
    "    test_files, dev_files, train_files = [], [], []\n",
    "    test_tokens = 0\n",
    "    dev_tokens = 0\n",
    "\n",
    "    for fname in all_files:\n",
    "        tokens = files_stats[fname]['tokens']\n",
    "        if test_tokens < target_test_tokens:\n",
    "            test_files.append(fname)\n",
    "            test_tokens += tokens\n",
    "        elif dev_tokens < target_dev_tokens:\n",
    "            dev_files.append(fname)\n",
    "            dev_tokens += tokens\n",
    "        else:\n",
    "            train_files.append(fname)\n",
    "\n",
    "    # Convert files to sentences and collect stats\n",
    "    def load_files_data(file_list):\n",
    "        all_sentences = []\n",
    "        stats = defaultdict(int)\n",
    "        entity_counts = defaultdict(int)\n",
    "\n",
    "        for fname in sorted(file_list):\n",
    "            file_path = os.path.join(input_dir, fname)\n",
    "            sentences = convert_file_to_bio_sentences(file_path)\n",
    "            all_sentences.extend(sentences)\n",
    "\n",
    "            file_stats = files_stats[fname]\n",
    "            stats['protocols'] += 1\n",
    "            stats['sentences'] += file_stats['sentences']\n",
    "            stats['tokens'] += file_stats['tokens']\n",
    "            stats['entities'] += file_stats['entities']  # already excludes EVENT/UNK\n",
    "\n",
    "            # Count entities by type from BIO (B- tags only)\n",
    "            for sentence in sentences:\n",
    "                for _, bio_tag in sentence:\n",
    "                    if bio_tag.startswith('B-'):\n",
    "                        etype = bio_tag[2:]\n",
    "                        entity_counts[etype] += 1\n",
    "\n",
    "        return all_sentences, dict(stats), dict(entity_counts)\n",
    "\n",
    "    train_data, train_stats, train_entities = load_files_data(train_files)\n",
    "    dev_data, dev_stats, dev_entities = load_files_data(dev_files)\n",
    "    test_data, test_stats, test_entities = load_files_data(test_files)\n",
    "\n",
    "    return {\n",
    "        'train': {'files': train_files, 'data': train_data, 'stats': train_stats, 'entities': train_entities},\n",
    "        'dev': {'files': dev_files, 'data': dev_data, 'stats': dev_stats, 'entities': dev_entities},\n",
    "        'test': {'files': test_files, 'data': test_data, 'stats': test_stats, 'entities': test_entities},\n",
    "    }\n",
    "\n",
    "\n",
    "def write_bio_file(sentences, output_file: str):\n",
    "    \"\"\"Write sentences to BIO format TSV file.\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            for token, bio_tag in sentence:\n",
    "                f.write(f\"{token}\\t{bio_tag}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def print_statistics_table(splits_data: dict):\n",
    "    \"\"\"Print a nice table with corpus statistics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORPUS STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"{'Split':<10} {'Protocols':<12} {'Sentences':<12} {'Tokens':<12} {'Entities':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for split_name in ['train', 'dev', 'test']:\n",
    "        stats = splits_data[split_name]['stats']\n",
    "        print(f\"{split_name.capitalize():<10} \"\n",
    "              f\"{stats.get('protocols', 0):<12} \"\n",
    "              f\"{stats.get('sentences', 0):<12} \"\n",
    "              f\"{stats.get('tokens', 0):<12} \"\n",
    "              f\"{stats.get('entities', 0):<12}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    total_protocols = sum(s['stats'].get('protocols', 0) for s in splits_data.values())\n",
    "    total_sentences = sum(s['stats'].get('sentences', 0) for s in splits_data.values())\n",
    "    total_tokens = sum(s['stats'].get('tokens', 0) for s in splits_data.values())\n",
    "    total_entities = sum(s['stats'].get('entities', 0) for s in splits_data.values())\n",
    "\n",
    "    print(f\"{'TOTAL':<10} \"\n",
    "          f\"{total_protocols:<12} \"\n",
    "          f\"{total_sentences:<12} \"\n",
    "          f\"{total_tokens:<12} \"\n",
    "          f\"{total_entities:<12}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PERCENTAGES (by tokens)\")\n",
    "    print(\"=\" * 80)\n",
    "    for split_name in ['train', 'dev', 'test']:\n",
    "        tokens = splits_data[split_name]['stats'].get('tokens', 0)\n",
    "        pct = 100 * tokens / total_tokens if total_tokens else 0.0\n",
    "        print(f\"{split_name.capitalize():<10} {pct:>6.2f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ENTITY COUNTS BY TYPE (EVENT/UNK excluded)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    all_types = set()\n",
    "    for split_data in splits_data.values():\n",
    "        all_types.update(split_data.get('entities', {}).keys())\n",
    "\n",
    "    print(f\"{'Type':<15} {'Train':<10} {'Dev':<10} {'Test':<10} {'Total':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for etype in sorted(all_types):\n",
    "        train_count = splits_data['train']['entities'].get(etype, 0)\n",
    "        dev_count = splits_data['dev']['entities'].get(etype, 0)\n",
    "        test_count = splits_data['test']['entities'].get(etype, 0)\n",
    "        total_count = train_count + dev_count + test_count\n",
    "        print(f\"{etype:<15} {train_count:<10} {dev_count:<10} {test_count:<10} {total_count:<10}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def save_file_distribution(splits_data: dict, output_file='file_distribution.txt'):\n",
    "    \"\"\"Save information about which files are in which split (entities exclude EVENT/UNK).\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"FILE DISTRIBUTION ACROSS SPLITS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"\\nNOTE: Total entities EXCLUDE types: {', '.join(sorted(EXCLUDED_ENTITY_TYPES))}\\n\\n\")\n",
    "\n",
    "        for split_name in ['train', 'dev', 'test']:\n",
    "            files = splits_data[split_name]['files']\n",
    "            stats = splits_data[split_name]['stats']\n",
    "\n",
    "            f.write(f\"{split_name.upper()} SET ({len(files)} files)\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            f.write(f\"Total tokens: {stats.get('tokens', 0)}\\n\")\n",
    "            f.write(f\"Total sentences: {stats.get('sentences', 0)}\\n\")\n",
    "            f.write(f\"Total entities: {stats.get('entities', 0)}\\n\\n\")\n",
    "\n",
    "            f.write(\"Files:\\n\")\n",
    "            for fname in sorted(files):\n",
    "                f.write(f\"  - {fname}\\n\")\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    print(f\" File distribution saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1.4: Splitting corpus into train/dev/test (EVENT/UNK excluded)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Random seed: {RANDOM_SEED}\")\n",
    "    print(f\"Excluded entity types: {', '.join(sorted(EXCLUDED_ENTITY_TYPES))}\")\n",
    "    print(f\"Target split: Train ~{100*(1-TEST_SIZE-DEV_SIZE):.0f}%, Dev ~{100*DEV_SIZE:.0f}%, Test ~{100*TEST_SIZE:.0f}%\")\n",
    "\n",
    "    splits_data = split_corpus(FILTERED_DIR, TEST_SIZE, DEV_SIZE, RANDOM_SEED)\n",
    "\n",
    "    print(\"Writing split files...\")\n",
    "    write_bio_file(splits_data['train']['data'], TRAIN_FILE)\n",
    "    print(f\"   {TRAIN_FILE}\")\n",
    "    write_bio_file(splits_data['dev']['data'], DEV_FILE)\n",
    "    print(f\"   {DEV_FILE}\")\n",
    "    write_bio_file(splits_data['test']['data'], TEST_FILE)\n",
    "    print(f\"   {TEST_FILE}\")\n",
    "\n",
    "    print_statistics_table(splits_data)\n",
    "\n",
    "    with open('corpus_statistics.txt', 'w', encoding='utf-8') as f:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = f\n",
    "        print_statistics_table(splits_data)\n",
    "        sys.stdout = old_stdout\n",
    "    print(f\" Statistics saved to: corpus_statistics.txt\")\n",
    "\n",
    "    save_file_distribution(splits_data, 'file_distribution.txt')\n",
    "\n",
    "    print(\"\\n Train/dev/test split complete!\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    print(f\"  - {TRAIN_FILE}\")\n",
    "    print(f\"  - {DEV_FILE}\")\n",
    "    print(f\"  - {TEST_FILE}\")\n",
    "    print(f\"  - corpus_statistics.txt\")\n",
    "    print(f\"  - file_distribution.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
