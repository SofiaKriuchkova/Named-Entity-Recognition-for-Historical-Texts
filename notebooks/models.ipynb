{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5294aa8c-d56b-4617-bd47-88c625b57319",
   "metadata": {},
   "source": [
    "# Experiments in training machine learning models, including data preparation, model training, and evaluation on a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0915c-4e9f-417c-8e1b-96d72c10fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['USE_TF'] = 'NO'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from nervaluate import Evaluator\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53b065-c64f-49d2-b4c2-ce0492624669",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7b387-4c92-4a20-b19a-c7fda38a54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load data in CoNLL format from a TSV file.\"\"\"\n",
    "    sentences = []\n",
    "    tokens, labels = [], []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Empty line indicates the end of a sentence\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    # Save the completed sentence\n",
    "                    sentences.append({\n",
    "                        'tokens': tokens,\n",
    "                        'ner_tags': labels\n",
    "                    })\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                # Split the line into token and label\n",
    "                parts = line.split('\\t')\n",
    "                \n",
    "                # Expect exactly two columns: token and NER tag\n",
    "                if len(parts) == 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    labels.append(parts[1])\n",
    "    \n",
    "    # Add the last sentence if the file does not end with a blank line\n",
    "    if tokens:\n",
    "        sentences.append({\n",
    "            'tokens': tokens,\n",
    "            'ner_tags': labels\n",
    "        })\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Load training and development datasets\n",
    "print(\"Loading data...\")\n",
    "train_data = load_data(\"train_w.tsv\")\n",
    "dev_data = load_data(\"dev_w.tsv\")\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Train: {len(train_data)} sentences\")\n",
    "print(f\"Dev: {len(dev_data)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e470a1b-9707-4a0a-9f3b-b6417894f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique labels\n",
    "all_labels = set()\n",
    "for example in train_data + dev_data:\n",
    "    all_labels.update(example['ner_tags'])\n",
    "\n",
    "label_list = sorted(list(all_labels))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"✓ Labels: {len(label_list)}\")\n",
    "print(f\"  {label_list}\")\n",
    "\n",
    "# ADDED: Extract entity types for nervaluate\n",
    "# Remove B- and I- prefixes to get entity types\n",
    "entity_types = set()\n",
    "for label in label_list:\n",
    "    if label != 'O':\n",
    "        entity_type = label.split('-')[1] if '-' in label else label\n",
    "        entity_types.add(entity_type)\n",
    "\n",
    "entity_types = sorted(list(entity_types))\n",
    "print(f\"\\n Entity types: {entity_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d9fe3-cdb7-4995-8870-d48575ae2eba",
   "metadata": {},
   "source": [
    "## Estbert: test and final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4d771-b2a1-4313-85bc-5ad9d34a9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '...'\n",
    "OUTPUT_DIR = '...' #path to the model's folder\n",
    "\n",
    "print(f\"\\nLoading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c94e0-2699-42fe-bec4-2da01871bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align(examples):\n",
    "    \"\"\"Tokenize input tokens and align NER labels with subword tokens.\"\"\"\n",
    "    \n",
    "    # Tokenize the input while keeping word boundaries\n",
    "    tokenized = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over each example in the batch\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        # Get mapping from tokens to original word indices\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        \n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "        \n",
    "        # Align labels with subword tokens\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens like [CLS], [SEP]\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                # First subword of a word gets the NER label\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                # Remaining subwords are ignored during loss computation\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            prev_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    # Attach labels to the tokenized output\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"Preparing datasets...\")\n",
    "\n",
    "# Create Hugging Face datasets from raw data\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(dev_data)\n",
    "\n",
    "# Apply tokenization and label alignment to training data\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Apply tokenization and label alignment to evaluation data\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Dataset is ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532e7e5-9b9f-47d5-bc38-1469df28ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New compute_metrics function using nervaluate\n",
    "def convert_to_entities(tags):\n",
    "    \"\"\"\n",
    "    Convert BIO tags to entity list format required by nervaluate.\n",
    "    \n",
    "    Args:\n",
    "        tags: List of BIO tags for one sentence\n",
    "    \n",
    "    Returns:\n",
    "        List of entity dictionaries with 'label', 'start', 'end'\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "        elif tag.startswith('B-'):\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            entity_type = tag[2:]  # Remove 'B-' prefix\n",
    "            current_entity = {\n",
    "                'label': entity_type,\n",
    "                'start': i,\n",
    "                'end': i + 1\n",
    "            }\n",
    "        elif tag.startswith('I-'):\n",
    "            if current_entity is not None:\n",
    "                current_entity['end'] = i + 1\n",
    "            else:\n",
    "                # I- tag without B- tag, treat as new entity\n",
    "                entity_type = tag[2:]  # Remove 'I-' prefix\n",
    "                current_entity = {\n",
    "                    'label': entity_type,\n",
    "                    'start': i,\n",
    "                    'end': i + 1\n",
    "                }\n",
    "    \n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute metrics using nervaluate.\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Convert predictions and labels to BIO tags\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Convert BIO tags to entity format for nervaluate\n",
    "    true_entities = [convert_to_entities(tags) for tags in true_labels]\n",
    "    pred_entities = [convert_to_entities(tags) for tags in true_predictions]\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = Evaluator(true_entities, pred_entities, tags=entity_types)\n",
    "    \n",
    "    results = evaluator.evaluate()\n",
    "    overall = results['overall']\n",
    "    \n",
    "    print(results.keys())\n",
    "    \n",
    "    # Extract strict metrics\n",
    "    strict_results = overall['strict']\n",
    "    #print(type(strict_results))\n",
    "    \n",
    "    return {\n",
    "        'precision': strict_results.precision,\n",
    "        'recall': strict_results.recall,\n",
    "        'f1': strict_results.f1,\n",
    "        # Optional: Add partial matching scores\n",
    "        'partial_precision': overall['partial'].precision,\n",
    "        'partial_recall': overall['partial'].recall,\n",
    "        'partial_f1': overall['partial'].f1,\n",
    "        \n",
    "    }\n",
    "\n",
    "print(\"Metrics function ready (using nervaluate)\")\n",
    "print(\"Evaluation scheme: strict (exact boundary + correct type)\")\n",
    "print(\"Aggregation: micro-average (default in nervaluate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82beac-4ca3-4704-b242-7c6a36d46440",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "def arguments(learning_r, batch):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/lr{learning_r}_bs{batch}\",\n",
    "        learning_rate=learning_r,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7e7ad-ca40-4293-90b9-0d0dbcc66050",
   "metadata": {},
   "source": [
    "Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52dc4de-6367-426f-b815-d95e9eed41c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [1e-5, 5e-6, 1e-6, 5e-5]\n",
    "batch_sizes = [16, 24, 32]\n",
    "results_list = []\n",
    "\n",
    "for l in learning_rates:\n",
    "    for b in batch_sizes:\n",
    "        trainer = Trainer(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                num_labels=len(label2id),\n",
    "                id2label=id2label,\n",
    "                label2id=label2id\n",
    "            ),\n",
    "            args=arguments(l, b),\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        print(\"Trainer created\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STARTING TRAINING - EstBERT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: {MODEL_NAME}\")\n",
    "        print(f\"Training samples: {len(train_data)}\")\n",
    "        print(f\"Dev samples: {len(dev_data)}\")\n",
    "        print(f\"Epochs: 3\")\n",
    "        print(f\"Batchsize:\", b)\n",
    "        print(f\"Learning rate:\", l)\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        print(\" Training complete!\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        results = trainer.evaluate()\n",
    "\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Precision: {results['eval_precision']:.4f}\")\n",
    "        print(f\"  Recall:    {results['eval_recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {results['eval_f1']:.4f}\")\n",
    "        print(f\"  Loss:      {results['eval_loss']:.4f}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        results_list.append({\n",
    "            \"learning_rate\": l,\n",
    "            \"batch_size\": b,\n",
    "            \"precision\": results[\"eval_precision\"],\n",
    "            \"recall\": results[\"eval_recall\"],\n",
    "            \"f1\": results[\"eval_f1\"],\n",
    "            \"loss\": results[\"eval_loss\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8834b1-4a7a-465a-9a23-1b41403c9df5",
   "metadata": {},
   "source": [
    "Looking for the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b8aa1-73d5-4dfb-b420-d6278ce963cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list).sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434ce4e-50c9-4667-92ad-897f57482594",
   "metadata": {},
   "source": [
    "Training with the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d56dfc-75d6-47bc-84f7-be10a1b6a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/best\",\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=50,  #limit\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    ),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=4,    # stop after N epochs without improvement\n",
    "            early_stopping_threshold=0.0\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Trainer created\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING - EstBERT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Dev samples: {len(dev_data)}\")\n",
    "print(f\"Epochs: ?\")\n",
    "print(f\"Batchsize: 16\")\n",
    "print(f\"Learning rate: 5e-05\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab6464-3cb4-4a7c-be3f-dad6a950a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Precision: {results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['eval_f1']:.4f}\")\n",
    "print(f\"  Loss:      {results['eval_loss']:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(f\"{OUTPUT_DIR}/results.txt\", 'w') as f:\n",
    "    f.write(f\"EstBERT NER Results\\n\")\n",
    "    f.write(f\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"Precision: {results['eval_precision']:.4f}\\n\")\n",
    "    f.write(f\"Recall: {results['eval_recall']:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {results['eval_f1']:.4f}\\n\")\n",
    "    f.write(f\"Loss: {results['eval_loss']:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/results.txt\")\n",
    "print(f\"Model saved to {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b616eb-fee3-445c-b203-2ac3c293c098",
   "metadata": {},
   "source": [
    "## Est-roberta: tests and final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4583c44-cda9-4ece-840e-5b7c703d63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_ROBERTA = '...'\n",
    "OUTPUT_DIR_ROBERTA = '...'\n",
    "\n",
    "print(f\"Loading {MODEL_NAME_ROBERTA}...\")\n",
    "\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(MODEL_NAME_ROBERTA)\n",
    "print(\"Est-RoBERTa loaded\")\n",
    "\n",
    "model_roberta = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME_ROBERTA,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5e6e7-a297-4171-9420-e27b6e856d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_roberta(examples):\n",
    "    \"\"\"Tokenize input tokens and align NER labels for RoBERTa-based models.\"\"\"\n",
    "    \n",
    "    # Tokenize while preserving word boundaries\n",
    "    tokenized = tokenizer_roberta(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over each example in the batch\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        # Map each subword token to its original word index\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        \n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "        \n",
    "        # Align word-level NER labels with subword tokens\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens (e.g. <s>, </s>)\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                # Assign label only to the first subword of each word\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                # Ignore subsequent subwords during loss computation\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            prev_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    # Attach aligned labels to the tokenized output\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"Preparing datasets for Est-RoBERTa...\")\n",
    "\n",
    "# Create Hugging Face datasets for RoBERTa\n",
    "train_dataset_roberta = Dataset.from_list(train_data)\n",
    "eval_dataset_roberta = Dataset.from_list(dev_data)\n",
    "\n",
    "# Apply tokenization and label alignment to training data\n",
    "train_dataset_roberta = train_dataset_roberta.map(\n",
    "    tokenize_and_align_roberta,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset_roberta.column_names\n",
    ")\n",
    "\n",
    "# Apply tokenization and label alignment to evaluation data\n",
    "eval_dataset_roberta = eval_dataset_roberta.map(\n",
    "    tokenize_and_align_roberta,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset_roberta.column_names\n",
    ")\n",
    "\n",
    "print(\"Datasets ready for Est-RoBERTa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e141886-aacb-4156-8440-9e44ae5cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_roberta(learning_r, batch):\n",
    "    training_args_roberta = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR_ROBERTA}/lr{learning_r}_bs{batch}\",\n",
    "        learning_rate=learning_r,\n",
    "        per_device_train_batch_size=batch, \n",
    "        per_device_eval_batch_size=batch,   \n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "        save_steps=500,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    return training_args_roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3fa03-493d-4984-9a4c-85105cc10091",
   "metadata": {},
   "source": [
    "Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e872a-e1f8-4293-9172-540b26f06283",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-5, 5e-6, 1e-6, 5e-5]\n",
    "batch_sizes = [16, 24, 32]\n",
    "results_list_roberta = []\n",
    "\n",
    "data_collator_roberta = DataCollatorForTokenClassification(tokenizer_roberta)\n",
    "compute_metrics_roberta = compute_metrics\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        trainer_roberta = Trainer(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(\n",
    "                MODEL_NAME_ROBERTA,\n",
    "                num_labels=len(label2id),\n",
    "                id2label=id2label,\n",
    "                label2id=label2id\n",
    "            ),\n",
    "            args=args_roberta(lr, bs),\n",
    "            train_dataset=train_dataset_roberta,\n",
    "            eval_dataset=eval_dataset_roberta,\n",
    "            tokenizer=tokenizer_roberta,\n",
    "            data_collator=data_collator_roberta,\n",
    "            compute_metrics=compute_metrics_roberta,\n",
    "        )\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STARTING TRAINING - Est-RoBERTa\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        \n",
    "        print(f\"Model: {MODEL_NAME_ROBERTA}\")\n",
    "        print(f\"Training samples: {len(train_data)}\")\n",
    "        print(f\"Dev samples: {len(dev_data)}\")\n",
    "        print(f\"Epochs: 3\")\n",
    "        print(f\"Batch size:\", bs)\n",
    "        print(f\"Learning rate:\", lr)\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        trainer_roberta.train()\n",
    "        \n",
    "        print(\"\\n✓ Est-RoBERTa training complete!\")\n",
    "        \n",
    "        print(\"✓ Est-RoBERTa trainer created\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION - Est-RoBERTa\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results_roberta = trainer_roberta.evaluate()\n",
    "        \n",
    "        print(f\"\\nEst-RoBERTa Results:\")\n",
    "        print(f\"  Precision: {results_roberta['eval_precision']:.4f}\")\n",
    "        print(f\"  Recall:    {results_roberta['eval_recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {results_roberta['eval_f1']:.4f}\")\n",
    "        print(f\"  Loss:      {results_roberta['eval_loss']:.4f}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        results_list_roberta.append({\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": bs,\n",
    "            \"precision\": results_roberta[\"eval_precision\"],\n",
    "            \"recall\": results_roberta[\"eval_recall\"],\n",
    "            \"f1\": results_roberta[\"eval_f1\"],\n",
    "            \"loss\": results_roberta[\"eval_loss\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4777f-4368-4d6e-bb51-b26f42f625ad",
   "metadata": {},
   "source": [
    "Looking for the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956d4a5-3a48-4c80-a0f6-bf0e1f0c7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list_roberta).sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83119ec-f8c3-438c-b9ac-e16449d5cedb",
   "metadata": {},
   "source": [
    "Training with the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa9c6a-a3a8-4ded-9c6f-18a87635bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR_ROBERTA}/best\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,   \n",
    "    num_train_epochs=50,          #limit\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",      \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer_roberta = Trainer(\n",
    "    model=model_roberta,\n",
    "    args=training_args_roberta,\n",
    "    train_dataset=train_dataset_roberta,\n",
    "    eval_dataset=eval_dataset_roberta,\n",
    "    tokenizer=tokenizer_roberta,\n",
    "    data_collator=data_collator_roberta,\n",
    "    compute_metrics=compute_metrics_roberta,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=4,   # stop after N epochs without improvement\n",
    "            early_stopping_threshold=0.0\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING - Est-RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME_ROBERTA}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Dev samples: {len(dev_data)}\")\n",
    "print(f\"Epochs: ?\")\n",
    "print(f\"Batch size: 16\")\n",
    "print(f\"Learning rate: 5e-5\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer_roberta.train()\n",
    "\n",
    "print(\"\\n Est-RoBERTa training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25459cb7-b35c-4f08-a85f-22a904d10329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION - Est-RoBERTa\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_roberta = trainer_roberta.evaluate()\n",
    "\n",
    "print(f\"\\nEst-RoBERTa Results:\")\n",
    "print(f\"  Precision: {results_roberta['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {results_roberta['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results_roberta['eval_f1']:.4f}\")\n",
    "print(f\"  Loss:      {results_roberta['eval_loss']:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "os.makedirs(OUTPUT_DIR_ROBERTA, exist_ok=True)\n",
    "with open(f\"{OUTPUT_DIR_ROBERTA}/results.txt\", 'w') as f:\n",
    "    f.write(f\"Est-RoBERTa NER Results\\n\")\n",
    "    f.write(f\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"Precision: {results_roberta['eval_precision']:.4f}\\n\")\n",
    "    f.write(f\"Recall: {results_roberta['eval_recall']:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {results_roberta['eval_f1']:.4f}\\n\")\n",
    "    f.write(f\"Loss: {results_roberta['eval_loss']:.4f}\\n\")\n",
    "\n",
    "print(f\"\\n Est-RoBERTa results saved to {OUTPUT_DIR_ROBERTA}/results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb2417-2535-4e4a-b19e-eb6a385be082",
   "metadata": {},
   "source": [
    "## Wikibert: tests and final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69b54d-b07e-45f8-b7ce-0f0281c6b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390570a6-547e-442c-8feb-abff5d534647",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '...'\n",
    "tokenizer = BertTokenizer.from_pretrained('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28b00d-26b8-4a11-9cd1-193673c5ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align(examples):\n",
    "    # Maximum sequence length for the model\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Lists to store processed inputs for the whole batch\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate over tokenized sentences and their NER tags\n",
    "    for words, tags in zip(examples[\"tokens\"], examples[\"ner_tags\"]):\n",
    "\n",
    "        # Start with [CLS] token\n",
    "        input_ids = [tokenizer.cls_token_id]\n",
    "        # Label -100 means \"ignore this token in loss computation\"\n",
    "        labels = [-100]\n",
    "\n",
    "        # Process each word and its corresponding NER tag\n",
    "        for word, tag in zip(words, tags):\n",
    "            # Tokenize the word into subword tokens\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # Skip words that produce no tokens\n",
    "            if not word_tokens:\n",
    "                continue\n",
    "\n",
    "            # Convert subword tokens to token IDs\n",
    "            word_token_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "\n",
    "            # Add subword token IDs to the input sequence\n",
    "            input_ids.extend(word_token_ids)\n",
    "\n",
    "            # Assign the label only to the first subword token\n",
    "            labels.append(label2id[tag])\n",
    "            # Ignore remaining subword tokens\n",
    "            labels.extend([-100] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Add [SEP] token at the end\n",
    "        input_ids.append(tokenizer.sep_token_id)\n",
    "        labels.append(-100)\n",
    "\n",
    "        # Attention mask: 1 for real tokens, 0 for padding\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Truncate sequences longer than MAX_LEN\n",
    "        if len(input_ids) > MAX_LEN:\n",
    "            input_ids = input_ids[:MAX_LEN]\n",
    "            attention_mask = attention_mask[:MAX_LEN]\n",
    "            labels = labels[:MAX_LEN]\n",
    "\n",
    "        # Calculate how much padding is needed\n",
    "        pad_len = MAX_LEN - len(input_ids)\n",
    "\n",
    "        # Pad input_ids, attention_mask, and labels to MAX_LEN\n",
    "        if pad_len > 0:\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask = attention_mask + [0] * pad_len\n",
    "            labels = labels + [-100] * pad_len\n",
    "\n",
    "        # Save processed example\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Return a dictionary compatible with Hugging Face datasets\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Create Hugging Face datasets from raw data\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(dev_data)\n",
    "\n",
    "# Apply tokenization and label alignment to the training dataset\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Apply tokenization and label alignment to the evaluation dataset\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0cb1a-3e12-4c2a-85c6-00dad2a25bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "def arguments(learning_r, batch):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/lr{learning_r}_bs{batch}\",\n",
    "        learning_rate=learning_r,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb890ab-8915-42d6-9cc4-3a8f63297256",
   "metadata": {},
   "source": [
    "Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d977c6-65b5-4c4f-bdf0-18b3a9e33e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-5, 5e-6, 1e-6, 5e-5]\n",
    "batch_sizes = [16, 24, 32]\n",
    "results_list = []\n",
    "\n",
    "for l in learning_rates:\n",
    "    for b in batch_sizes:\n",
    "        trainer = Trainer(\n",
    "            BertForTokenClassification.from_pretrained(\n",
    "                '...',\n",
    "                num_labels=len(label2id),\n",
    "                id2label=id2label,\n",
    "                label2id=label2id\n",
    "            ),\n",
    "            args=arguments(l, b),\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        print(\"Trainer created\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STARTING TRAINING - EstBERT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: TurkuNLP/wikibert-base-et-cased\")\n",
    "        print(f\"Training samples: {len(train_data)}\")\n",
    "        print(f\"Dev samples: {len(dev_data)}\")\n",
    "        print(f\"Epochs: 3\")\n",
    "        print(f\"Batchsize:\", b)\n",
    "        print(f\"Learning rate:\", l)\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        print(\"\\n Training complete!\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        results = trainer.evaluate()\n",
    "\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Precision: {results['eval_precision']:.4f}\")\n",
    "        print(f\"  Recall:    {results['eval_recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {results['eval_f1']:.4f}\")\n",
    "        print(f\"  Loss:      {results['eval_loss']:.4f}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        results_list.append({\n",
    "            \"learning_rate\": l,\n",
    "            \"batch_size\": b,\n",
    "            \"precision\": results[\"eval_precision\"],\n",
    "            \"recall\": results[\"eval_recall\"],\n",
    "            \"f1\": results[\"eval_f1\"],\n",
    "            \"loss\": results[\"eval_loss\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17aaa2-87b6-42af-a2d9-bd5ea843ebbc",
   "metadata": {},
   "source": [
    "Looking for the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0eb180-d6ea-4966-9910-47d2366d8ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list).sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc6323-44f3-4f82-a57e-6ff0e565b403",
   "metadata": {},
   "source": [
    "Training with the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82aea41-e1ed-4743-9b42-cefd2bc3e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/best\",\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    num_train_epochs=50,          #limit\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=BertForTokenClassification.from_pretrained(\n",
    "        \"TurkuNLP/wikibert-base-et-cased\",\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    ),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=4,\n",
    "            early_stopping_threshold=0.0\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Trainer created\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING - EstBERT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: wikibert\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Dev samples: {len(dev_data)}\")\n",
    "print(f\"Epochs: ?\")\n",
    "print(f\"Batchsize: 24 \")\n",
    "print(f\"Learning rate: 5e-05\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c33d2-22cf-47ec-9fb0-3cd46da7a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Precision: {results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['eval_f1']:.4f}\")\n",
    "print(f\"  Loss:      {results['eval_loss']:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(f\"{OUTPUT_DIR}/results.txt\", 'w') as f:\n",
    "    f.write(f\"EstBERT NER Results\\n\")\n",
    "    f.write(f\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"Precision: {results['eval_precision']:.4f}\\n\")\n",
    "    f.write(f\"Recall: {results['eval_recall']:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {results['eval_f1']:.4f}\\n\")\n",
    "    f.write(f\"Loss: {results['eval_loss']:.4f}\\n\")\n",
    "\n",
    "print(f\"\\n Results saved to {OUTPUT_DIR}/results.txt\")\n",
    "print(f\"Model saved to {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30b30a-fab6-4944-9684-4070ab6f0040",
   "metadata": {},
   "source": [
    "## Evaluation of models trained on a set of non-normalised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd97a9b-8817-4952-a192-4bb233a8d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc61b8-60d5-4f1d-82b0-7dcd69bf0221",
   "metadata": {},
   "source": [
    "### Estbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710dce1-806b-4c8e-b4de-0d14cb044a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"...\"  # checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "model.eval()  # evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11713cd8-9b19-4284-8c81-c7a53621d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data('...')\n",
    "\n",
    "def tokenize_and_align(examples):\n",
    "    \"\"\"Tokenize and align labels.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,  \n",
    "        max_length=512  # max length\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b3d72-0e88-44dc-8868-4336d09b8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute metrics using nervaluate.\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Convert predictions and labels to BIO tags\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Convert BIO tags to entity format for nervaluate\n",
    "    true_entities = [convert_to_entities(tags) for tags in true_labels]\n",
    "    pred_entities = [convert_to_entities(tags) for tags in true_predictions]\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = Evaluator(true_entities, pred_entities, tags=entity_types)\n",
    "    results = evaluator.evaluate()\n",
    "    \n",
    "    overall = results['overall']\n",
    "    entities = results['entities']\n",
    "    \n",
    "    strict = overall['strict']\n",
    "    partial = overall['partial']\n",
    "\n",
    "    metrics = {\n",
    "        # common metrics\n",
    "        'precision': strict.precision,\n",
    "        'recall': strict.recall,\n",
    "        'f1': strict.f1,\n",
    "        'partial_precision': partial.precision,\n",
    "        'partial_recall': partial.recall,\n",
    "        'partial_f1': partial.f1,\n",
    "    }\n",
    "\n",
    "    # entities\n",
    "    for ent_type in entity_types:\n",
    "        if ent_type in entities:\n",
    "            m = entities[ent_type]['strict']\n",
    "            metrics[f'{ent_type}_precision'] = m.precision\n",
    "            metrics[f'{ent_type}_recall'] = m.recall\n",
    "            metrics[f'{ent_type}_f1'] = m.f1\n",
    "        else:\n",
    "            metrics[f'{ent_type}_precision'] = 0.0\n",
    "            metrics[f'{ent_type}_recall'] = 0.0\n",
    "            metrics[f'{ent_type}_f1'] = 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\" Metrics function ready (using nervaluate)\")\n",
    "print(\"  Evaluation scheme: strict (exact boundary + correct type)\")\n",
    "print(\"  Aggregation: micro-average (default in nervaluate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a0def-2582-4725-9fb4-0aa5eb32c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "print(\"Preparing test dataset...\")\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")\n",
    "print(\" Test dataset ready\")\n",
    "\n",
    "# data collator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100 #standart parameter\n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION ON THE TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# RESULTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON THE TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nStrict matching (exact boundary + correct type):\")\n",
    "print(f\"  Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nPartial matching:\")\n",
    "print(f\"  Precision: {test_results['eval_partial_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_partial_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nLoss: {test_results['eval_loss']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce1a58-2486-4404-ac0b-1535395e4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Strict F1: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Partial F1: {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-entity results (strict) ===\")\n",
    "for ent in entity_types:\n",
    "    print(f\"{ent:10s}  P={test_results.get(f'eval_{ent}_precision', 0):.4f}  \"\n",
    "          f\"R={test_results.get(f'eval_{ent}_recall', 0):.4f}  \"\n",
    "          f\"F1={test_results.get(f'eval_{ent}_f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9861c8b-b77d-45fb-a5c9-235eba3d3c8b",
   "metadata": {},
   "source": [
    "### Est-roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be73911-749c-482e-86a3-5c9c65e26601",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '...'  # checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "model.eval()  # evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68689f-8eb9-45f2-ab3b-132a68734d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "print(\"Preparing test dataset...\")\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")\n",
    "print(\"Test dataset ready\")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100 \n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"START EVALUATION ON THE TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON THE TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nStrict matching (exact boundary + correct type):\")\n",
    "print(f\"  Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nPartial matching:\")\n",
    "print(f\"  Precision: {test_results['eval_partial_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_partial_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nLoss: {test_results['eval_loss']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b132b5-12e2-42f8-982c-33a4e1246d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Strict F1: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Partial F1: {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-entity results (strict) ===\")\n",
    "for ent in entity_types:\n",
    "    print(f\"{ent:10s}  P={test_results.get(f'eval_{ent}_precision', 0):.4f}  \"\n",
    "          f\"R={test_results.get(f'eval_{ent}_recall', 0):.4f}  \"\n",
    "          f\"F1={test_results.get(f'eval_{ent}_f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f86bb-4491-4626-9561-3f221089ff67",
   "metadata": {},
   "source": [
    "### Wikibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f175a34-0401-4a0c-8509-df0d1a4d86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '...'  #checkpoint\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd63e16-23c8-4145-a1ec-73e155bd82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align(examples):\n",
    "    # Maximum sequence length for the model\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # Lists to store processed inputs for the whole batch\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate over tokenized sentences and their NER tags\n",
    "    for words, tags in zip(examples[\"tokens\"], examples[\"ner_tags\"]):\n",
    "\n",
    "        # Start with [CLS] token\n",
    "        input_ids = [tokenizer.cls_token_id]\n",
    "        # Label -100 means \"ignore this token in loss computation\"\n",
    "        labels = [-100]\n",
    "\n",
    "        # Process each word and its corresponding NER tag\n",
    "        for word, tag in zip(words, tags):\n",
    "            # Tokenize the word into subword tokens\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # Skip words that produce no tokens\n",
    "            if not word_tokens:\n",
    "                continue\n",
    "\n",
    "            # Convert subword tokens to token IDs\n",
    "            word_token_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "\n",
    "            # Add subword token IDs to the input sequence\n",
    "            input_ids.extend(word_token_ids)\n",
    "\n",
    "            # Assign the label only to the first subword token\n",
    "            labels.append(label2id[tag])\n",
    "            # Ignore remaining subword tokens\n",
    "            labels.extend([-100] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Add [SEP] token at the end\n",
    "        input_ids.append(tokenizer.sep_token_id)\n",
    "        labels.append(-100)\n",
    "\n",
    "        # Attention mask: 1 for real tokens, 0 for padding\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Truncate sequences longer than MAX_LEN\n",
    "        if len(input_ids) > MAX_LEN:\n",
    "            input_ids = input_ids[:MAX_LEN]\n",
    "            attention_mask = attention_mask[:MAX_LEN]\n",
    "            labels = labels[:MAX_LEN]\n",
    "\n",
    "        # Calculate how much padding is needed\n",
    "        pad_len = MAX_LEN - len(input_ids)\n",
    "\n",
    "        # Pad input_ids, attention_mask, and labels to MAX_LEN\n",
    "        if pad_len > 0:\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask = attention_mask + [0] * pad_len\n",
    "            labels = labels + [-100] * pad_len\n",
    "\n",
    "        # Save processed example\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Return a dictionary compatible with Hugging Face datasets\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Create Hugging Face datasets from raw data\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(dev_data)\n",
    "\n",
    "# Apply tokenization and label alignment to the training dataset\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Apply tokenization and label alignment to the evaluation dataset\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415393f8-6493-4fd7-b1fd-57589f14d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "print(\"Preparing test dataset...\")\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_and_align,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")\n",
    "print(\"Test dataset ready\")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100  \n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nStrict matching (exact boundary + correct type):\")\n",
    "print(f\"  Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nPartial matching:\")\n",
    "print(f\"  Precision: {test_results['eval_partial_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_partial_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nLoss: {test_results['eval_loss']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce34f80-af4c-4660-99e7-bc1d298b1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Strict F1: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Partial F1: {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-entity results (strict) ===\")\n",
    "for ent in entity_types:\n",
    "    print(f\"{ent:10s}  P={test_results.get(f'eval_{ent}_precision', 0):.4f}  \"\n",
    "          f\"R={test_results.get(f'eval_{ent}_recall', 0):.4f}  \"\n",
    "          f\"F1={test_results.get(f'eval_{ent}_f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df297e3e-0fcf-4b89-ae5e-9c2f1b304622",
   "metadata": {},
   "source": [
    "### EstBERT_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc89f04-0872-4d23-805a-4ede98d63098",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data('...')\n",
    "\n",
    "def tokenize_and_align(examples):\n",
    "    print(\"CALLED tokenize_and_align, sample tags:\", examples[\"ner_tags\"][0][:5])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                tag = label_seq[word_idx]\n",
    "                if tag not in model.config.label2id:\n",
    "                    tag = \"O\"\n",
    "                label_ids.append(model.config.label2id[tag])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized = dict(tokenized)     \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2ca5b-f5be-4aad-b2c0-431f90a5c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_entities(tags):\n",
    "    \"\"\"\n",
    "    Convert BIO tags to entity list format required by nervaluate.\n",
    "    \n",
    "    Args:\n",
    "        tags: List of BIO tags for one sentence\n",
    "    \n",
    "    Returns:\n",
    "        List of entity dictionaries with 'label', 'start', 'end'\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "        elif tag.startswith('B-'):\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            entity_type = tag[2:]  # Remove 'B-' prefix\n",
    "            current_entity = {\n",
    "                'label': entity_type,\n",
    "                'start': i,\n",
    "                'end': i + 1\n",
    "            }\n",
    "        elif tag.startswith('I-'):\n",
    "            if current_entity is not None:\n",
    "                current_entity['end'] = i + 1\n",
    "            else:\n",
    "                # I- tag without B- tag, treat as new entity\n",
    "                entity_type = tag[2:]  # Remove 'I-' prefix\n",
    "                current_entity = {\n",
    "                    'label': entity_type,\n",
    "                    'start': i,\n",
    "                    'end': i + 1\n",
    "                }\n",
    "    \n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "    \n",
    "def compute_metrics(p):\n",
    "    import numpy as np\n",
    "    from nervaluate import Evaluator\n",
    "\n",
    "    logits, labels = p\n",
    "    preds = np.argmax(logits, axis=2)\n",
    "\n",
    "    id2label = model.config.id2label\n",
    "\n",
    "    # entities PER/ORG/LOC...\n",
    "    entity_types = sorted({\n",
    "        lab.split(\"-\", 1)[1] for lab in id2label.values() if \"-\" in lab\n",
    "    })\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[int(pred)] for (pred, lab) in zip(pred_row, lab_row) if lab != -100]\n",
    "        for pred_row, lab_row in zip(preds, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[int(lab)] for (pred, lab) in zip(pred_row, lab_row) if lab != -100]\n",
    "        for pred_row, lab_row in zip(preds, labels)\n",
    "    ]\n",
    "\n",
    "    true_entities = [convert_to_entities(tags) for tags in true_labels]\n",
    "    pred_entities = [convert_to_entities(tags) for tags in true_predictions]\n",
    "\n",
    "    evaluator = Evaluator(true_entities, pred_entities, tags=entity_types)\n",
    "    results = evaluator.evaluate()\n",
    "\n",
    "    overall = results[\"overall\"]\n",
    "    entities = results[\"entities\"]\n",
    "    strict = overall[\"strict\"]\n",
    "    partial = overall[\"partial\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"precision\": strict.precision,\n",
    "        \"recall\": strict.recall,\n",
    "        \"f1\": strict.f1,\n",
    "        \"partial_precision\": partial.precision,\n",
    "        \"partial_recall\": partial.recall,\n",
    "        \"partial_f1\": partial.f1,\n",
    "    }\n",
    "\n",
    "    for ent_type in entity_types:\n",
    "        if ent_type in entities:\n",
    "            m = entities[ent_type][\"strict\"]\n",
    "            metrics[f\"{ent_type}_precision\"] = m.precision\n",
    "            metrics[f\"{ent_type}_recall\"] = m.recall\n",
    "            metrics[f\"{ent_type}_f1\"] = m.f1\n",
    "        else:\n",
    "            metrics[f\"{ent_type}_precision\"] = 0.0\n",
    "            metrics[f\"{ent_type}_recall\"] = 0.0\n",
    "            metrics[f\"{ent_type}_f1\"] = 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "print(\" Universal metrics function ready (nervaluate-based)\")\n",
    "print(\"   Auto-detects labels from model\")\n",
    "print(\"   Evaluation scheme: strict + partial\")\n",
    "print(\"   Aggregation: micro-average (nervaluate default)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb40dcf-d0bc-4078-b2a9-86ae444ee8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '...'\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"num_labels:\", model.config.num_labels)\n",
    "print(\"label2id:\", model.config.label2id)\n",
    "print(\"id2label:\", model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4199df9-f8a9-49d8-a69d-f2ca8982f131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "print(\"Preparing test dataset...\")\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "test_dataset = test_dataset.map(tokenize_and_align, batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "print(\"Test dataset ready\")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100 \n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nStrict matching (exact boundary + correct type):\")\n",
    "print(f\"  Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nPartial matching:\")\n",
    "print(f\"  Precision: {test_results['eval_partial_precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_results['eval_partial_recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nLoss: {test_results['eval_loss']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72713ed9-dff9-40f3-bf83-7a24ad836ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Overall ===\")\n",
    "print(f\"Strict F1: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Partial F1: {test_results['eval_partial_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-entity results (strict) ===\")\n",
    "for ent in entity_types:\n",
    "    print(f\"{ent:10s}  P={test_results.get(f'eval_{ent}_precision', 0):.4f}  \"\n",
    "          f\"R={test_results.get(f'eval_{ent}_recall', 0):.4f}  \"\n",
    "          f\"F1={test_results.get(f'eval_{ent}_f1', 0):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
